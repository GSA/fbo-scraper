{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sqlalchemy import create_engine, func, case, inspect\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.pool import NullPool\n",
    "from sqlalchemy_utils import database_exists, create_database, drop_database\n",
    "\n",
    "import utils.db.db as db\n",
    "from utils.db.db_utils import get_db_url, session_scope, DataAccessLayer, insert_data, fetch_notice_type_id\n",
    "\n",
    "\n",
    "conn_string = \"postgresql://circleci:srtpass@localhost/srt\"\n",
    "conn_string = \"postgresql://urx0lzh5ex4u9pgg:1zfcf2tt8xrc4xu4oj1znf8og@localhost:40169/cgawsbrokerprodiqytzz7z2ihzbad\"\n",
    "\n",
    "\n",
    "dal = DataAccessLayer(conn_string)\n",
    "dal.connect()\n",
    "\n",
    "ntype = 'Combined Synopsis/Solicitation'\n",
    "\n",
    "#with session_scope(dal) as session:\n",
    "#    attachments = session.query(db.Attachment)\n",
    "#    print (attachments)\n",
    "#    alist = [a for a in attachments]\n",
    "    \n",
    "#    print (alist[0].attachment_text)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in with\n",
      "select notice_id, attachment_text \n",
      "              from \"Predictions\" p\n",
      "              join notice n on p.\"solNum\" = n.solicitation_number\n",
      "              join attachment a on a.notice_id = n.id \n",
      "              where p.na_flag  limit 10 \n",
      "10\n",
      "(19066, 'UNCLASSIFIED\\n\\nSynopsis for Commercial Items\\n19304AS69\\nThis is combined synopsis/solicit\n",
      "(28563, 'Subject: RFQ No. 7200AA20Q00029\\nAmendment Number: One (1)\\nIssuance Date of Amendment Numb\n",
      "(28563, '')\n",
      "(30617, 'Subject: RFQ No. 7200AA20Q00029\\nAmendment Number: One (1)\\nIssuance Date of Amendment Numb\n",
      "(30617, '')\n",
      "(33282, '1. REQUISITION NUMBER\\n\\nSOLICITATION/CONTRACT/ORDER FOR COMMERCIAL ITEMS\\n2. CONTRACT NO.\\\n",
      "(33282, 'Attachment\\nFAR Deviation Text\\nBaseline is FAC 2020-05, published in the Federal Register \n",
      "(33282, \"Attachment A\\n\\nCAAC Letter 2020-05 FAR text 2020-014\\nUnited States-Mexico-Canada Agreemen\n",
      "(33282, \"Please fill out the matrix below and return a sign copy as part of the submission. The Vend\n",
      "(34023, 'This FAR Clause requires a response in section (d) 1 and 2 that must be checked and returne\n"
     ]
    }
   ],
   "source": [
    "with dal.engine.connect() as con:\n",
    "    print (\"in with\")\n",
    "    sql = \"\"\"select notice_id, attachment_text \n",
    "              from \"Predictions\" p\n",
    "              join notice n on p.\"solNum\" = n.solicitation_number\n",
    "              join attachment a on a.notice_id = n.id \n",
    "              where p.na_flag  limit 10 \"\"\"\n",
    "    print (sql)\n",
    "    rs = con.execute( sql )    \n",
    "    print (rs.rowcount)\n",
    "    for row in rs:\n",
    "        print (str(row)[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/crowley/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "no_nonsense_re = re.compile(r'^[a-zA-Z^508]+$')\n",
    "\n",
    "def strip_nonsense(doc):\n",
    "    \"\"\"\n",
    "    Returns stemmed lowercased alpha-only substrings from a string that are b/w 3 and 17 chars long. \n",
    "    It keeps the substring `508`.\n",
    "    \n",
    "    Parameters:\n",
    "        doc (str): the text of a single FBO document.\n",
    "        \n",
    "    Returns:\n",
    "        words (str): a string of space-delimited lower-case alpha-only words (except for `508`)\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    doc = doc.split()\n",
    "    words = ''\n",
    "    for word in doc:\n",
    "        m = re.match(no_nonsense_re, word)\n",
    "        if m:\n",
    "            match = m.group()\n",
    "            if match in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                match_len = len(match)\n",
    "                if match_len <= 17 and match_len >= 3:\n",
    "                    porter = PorterStemmer()\n",
    "                    stemmed = porter.stem(match)\n",
    "                    words += stemmed + ' '\n",
    "    return words\n",
    "\n",
    "def getData(dal, sql, target):\n",
    "    \"\"\"\n",
    "    Returns an array of dictionaries. \n",
    "    Each dictionary represents one solicitaiton. It has 2 keys: 'text' and 'target'\n",
    "    The text is the stemmed word list from the solication attachments\n",
    "    The target is 1 or Not Applicable and 0 for Applicable\n",
    "    \"\"\"\n",
    "    SolData = {}\n",
    "\n",
    "    with dal.engine.connect() as con:\n",
    "        rs = con.execute( sql )    \n",
    "        for row in rs:\n",
    "            if row.attachment_text:\n",
    "                if row.solNum not in SolData:\n",
    "                    SolData[row.solNum] = row.attachment_text\n",
    "                else:\n",
    "                    SolData[row.solNum] += row.attachment_text\n",
    "                \n",
    "\n",
    "    NAData = []\n",
    "    for key in SolData.keys():\n",
    "        NAData.append ( { 'text': strip_nonsense(SolData[key]), 'target': target } )\n",
    "    \n",
    "    return NAData\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def getNAData(dal):\n",
    "    sql = \"\"\"select attachment_text, \"solNum\", notice_id, a.id\n",
    "              from \"Predictions\" p\n",
    "              join notice n on p.\"solNum\" = n.solicitation_number\n",
    "              join attachment a on a.notice_id = n.id \n",
    "              where p.na_flag\n",
    "              limit 1000 \"\"\"\n",
    "    \n",
    "    return getData(dal, sql, 1)\n",
    "\n",
    "def getApplicableData(dal):\n",
    "    sql = \"\"\"select attachment_text, \"solNum\", notice_id, a.id\n",
    "              from \"Predictions\" p\n",
    "              join notice n on p.\"solNum\" = n.solicitation_number\n",
    "              join attachment a on a.notice_id = n.id \n",
    "              where p.predictions->>'value' = 'green' \n",
    "              limit 1000 \"\"\"\n",
    "\n",
    "    sql2 = \"\"\"select count(*)\n",
    "          from \"Predictions\" p\n",
    "          join notice n on p.\"solNum\" = n.solicitation_number\n",
    "          join attachment a on a.notice_id = n.id \n",
    "          where p.predictions->>'value' = 'green' \n",
    "          limit 1000\n",
    "           \"\"\"\n",
    "\n",
    "    return getData(dal, sql, 0)\n",
    "    \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 NA entries\n",
      "Found 112 A entries\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "na_data = getNAData(dal)\n",
    "print (\"Found {} NA entries\".format(len(na_data)))\n",
    "a_data = getApplicableData(dal)\n",
    "print (\"Found {} A entries\".format(len(a_data)))\n",
    "\n",
    "full_data = na_data + a_data\n",
    "\n",
    "print (full_data[0]['target'])\n",
    "print (full_data[20]['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump( full_data, open('full_data.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ead7231b6066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Allow import from parent directory (for utils for example)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattachments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.train'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "import dill as pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Allow import from parent directory (for utils for example)\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import utils.train as train\n",
    "\n",
    "x,y = train.prepare_samples(attachments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
