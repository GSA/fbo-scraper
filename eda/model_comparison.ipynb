{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectPercentile, SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "#in order to use SMOTE, you've got to import Pipeline from imblearn\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpickle Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['best_clf_scott_accuracy.pkl', 'best_clf_scott_fbeta.pkl', \n",
    "          'best_clf_scott_precision.pkl','best_clf_scott_roc_auc.pkl']\n",
    "model_map = {k:None for k in models}\n",
    "for m in models:\n",
    "    with open(m, 'rb') as f:\n",
    "        pickled_model = pickle.load(f)\n",
    "        model_map[m] = pickled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RED_FA8773-10-R-0086.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSTATEMENT OF W...</td>\n",
       "      <td>RED</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RED_NAMA-10-Q-0119.txt</td>\n",
       "      <td>\\nThis is a combined synopsis/solicitation for...</td>\n",
       "      <td>RED</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RED_EA1330-12-RQ-0249.txt</td>\n",
       "      <td>AMENDMENT OF SOLICITATION/MODIFICATION OF CONT...</td>\n",
       "      <td>RED</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RED_FA7014-12-T-1016.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSAF/FMB \\n\\n\\n\\n\\n...</td>\n",
       "      <td>RED</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GREEN_1055521.txt</td>\n",
       "      <td>\\n\\nStatement of Work:\\n\\n1.0   BACKGROUND\\nFD...</td>\n",
       "      <td>GREEN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file  \\\n",
       "0   RED_FA8773-10-R-0086.txt   \n",
       "1     RED_NAMA-10-Q-0119.txt   \n",
       "2  RED_EA1330-12-RQ-0249.txt   \n",
       "3   RED_FA7014-12-T-1016.txt   \n",
       "4          GREEN_1055521.txt   \n",
       "\n",
       "                                                text  label  target  \n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSTATEMENT OF W...    RED       1  \n",
       "1  \\nThis is a combined synopsis/solicitation for...    RED       1  \n",
       "2  AMENDMENT OF SOLICITATION/MODIFICATION OF CONT...    RED       1  \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSAF/FMB \\n\\n\\n\\n\\n...    RED       1  \n",
       "4  \\n\\nStatement of Work:\\n\\n1.0   BACKGROUND\\nFD...  GREEN       0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_labeled_df(labeled_data_path):\n",
    "    '''\n",
    "    Create a pandas DataFrame with the labled attachment texts.\n",
    "    \n",
    "    Arguments:\n",
    "        labeled_data_path (str): the directory for the labeled attachment text files.\n",
    "        \n",
    "    Returns:\n",
    "        labeled_df (pandas DataFrame): a dataframe with a column for the file name, \n",
    "                                       the text, and the label (green, yellow or red).\n",
    "    '''\n",
    "    \n",
    "    texts = []\n",
    "    files = []\n",
    "    labels = []\n",
    "    for file in os.listdir(labeled_data_path):\n",
    "        if file.startswith('.'):\n",
    "            continue\n",
    "        else:\n",
    "            files.append(file)\n",
    "            label = file.split('_')[0]\n",
    "            labels.append(label)\n",
    "            file_path = os.path.join(labeled_data_path,file)\n",
    "            #foce utf-8, ignoring erros\n",
    "            with open(file_path, 'r', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "                texts.append(text)\n",
    "    labeled_df = pd.DataFrame(data=[files,texts,labels]).transpose()\n",
    "    labeled_df.columns = ['file','text','label']\n",
    "    \n",
    "    return labeled_df\n",
    "\n",
    "labeled_df = create_labeled_df('labeled_fbo_docs')\n",
    "#recode labels to numeric\n",
    "labeled_df['target'] = labeled_df['label'].map({'GREEN':0,'YELLOW':1,'RED':1})\n",
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "no_nonsense_re = re.compile(r'^[a-zA-Z^508]+$')\n",
    "def strip_nonsense(doc):\n",
    "    \"\"\"\n",
    "    Returns lowercased substrings from a string that are at least 3 characters long, do not contain a number, and \n",
    "    are no more than 17 chars long.\n",
    "    \"\"\"\n",
    "    doc = doc.lower()\n",
    "    doc = doc.split()\n",
    "    words = ''\n",
    "    for word in doc:\n",
    "        m = re.match(no_nonsense_re, word)\n",
    "        if m:\n",
    "            match = m.group()\n",
    "            if match in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                match_len = len(match)\n",
    "                if match_len <= 17 and match_len >= 3:\n",
    "                    porter = PorterStemmer()\n",
    "                    stemmed = porter.stem(match)\n",
    "                    words += match + ' '\n",
    "    return words\n",
    "\n",
    "labeled_df['normalized_text'] = labeled_df['text'].apply(strip_nonsense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results of Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clf_report(models, X_test, y_test):\n",
    "    for k, clf in models.items():\n",
    "        print(\"=\"*80)\n",
    "        print(k)\n",
    "        print(\"=\"*80)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(metrics.classification_report(y_test, y_pred, target_names=['green', 'red']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "best_clf_scott_accuracy.pkl\n",
      "================================================================================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      green       0.82      0.57      0.67        54\n",
      "        red       0.86      0.95      0.90       145\n",
      "\n",
      "avg / total       0.85      0.85      0.84       199\n",
      "\n",
      "================================================================================\n",
      "best_clf_scott_fbeta.pkl\n",
      "================================================================================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      green       0.84      0.39      0.53        54\n",
      "        red       0.81      0.97      0.88       145\n",
      "\n",
      "avg / total       0.82      0.81      0.79       199\n",
      "\n",
      "================================================================================\n",
      "best_clf_scott_precision.pkl\n",
      "================================================================================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      green       0.90      0.33      0.49        54\n",
      "        red       0.80      0.99      0.88       145\n",
      "\n",
      "avg / total       0.83      0.81      0.78       199\n",
      "\n",
      "================================================================================\n",
      "best_clf_scott_roc_auc.pkl\n",
      "================================================================================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      green       0.61      0.80      0.69        54\n",
      "        red       0.91      0.81      0.86       145\n",
      "\n",
      "avg / total       0.83      0.81      0.82       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = labeled_df['normalized_text']\n",
    "y = labeled_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=123)\n",
    "\n",
    "print_model_results(model_map, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier with `roc_auc` as the scorer is best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results to Dummy Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dummy_clf_report(X_train, y_train, X_test, y_test):\n",
    "    for strategy in ['stratified','most_frequent','prior','uniform']:\n",
    "        print(\"=\"*80)\n",
    "        print(strategy)\n",
    "        print(\"=\"*80)\n",
    "        dummy = DummyClassifier(strategy=strategy)\n",
    "        dummy.fit(X_train, y_train)\n",
    "        y_pred = dummy.predict(X_test)\n",
    "        print(metrics.classification_report(y_test, y_pred, target_names=['green', 'red']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "stratified\n",
      "================================================================================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      green       0.30      0.24      0.27        54\n",
      "        red       0.74      0.79      0.76       145\n",
      "\n",
      "avg / total       0.62      0.64      0.63       199\n",
      "\n",
      "================================================================================\n",
      "most_frequent\n",
      "================================================================================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      green       0.00      0.00      0.00        54\n",
      "        red       0.73      1.00      0.84       145\n",
      "\n",
      "avg / total       0.53      0.73      0.61       199\n",
      "\n",
      "================================================================================\n",
      "prior\n",
      "================================================================================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      green       0.00      0.00      0.00        54\n",
      "        red       0.73      1.00      0.84       145\n",
      "\n",
      "avg / total       0.53      0.73      0.61       199\n",
      "\n",
      "================================================================================\n",
      "uniform\n",
      "================================================================================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      green       0.29      0.46      0.35        54\n",
      "        red       0.74      0.57      0.65       145\n",
      "\n",
      "avg / total       0.62      0.54      0.57       199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros(shape=labeled_df.shape)\n",
    "y = labeled_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=123)\n",
    "print_dummy_clf_report(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
